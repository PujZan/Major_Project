{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "CUDA Device Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# cuda running??\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom ResNet model with added dropout and a fully connected layer\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)  # Load pre-trained ResNet101 model\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.model.fc.in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Focal Loss function for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        self.ce = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ce_loss = self.ce(logits, labels)  # Calculate cross-entropy loss\n",
    "        p_t = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss  # Apply focal loss formula\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for loading lung images\n",
    "class LungImageDataset(Dataset):\n",
    "    def __init__(self, dataset_folder, transform=None):\n",
    "        self.dataset_folder = dataset_folder #Stores the path to the dataset folder\n",
    "        self.transform = transform #Stores the transformation function in the instance variable transform\n",
    "        self.data = self._load_data(dataset_folder)\n",
    "\n",
    "    def _load_data(self, dataset_folder):\n",
    "        # Load image paths and labels from the specified folder\n",
    "        data = []\n",
    "        labels = []\n",
    "        class_names = ['Pneumonia','Atelectasis','Cardiomegaly', 'Consolidation','Edema','Effusion', 'Emphysema','Fibrosis','Infiltration','Mass','Nodule','Pleural_Thickening','Pneumothorax', 'Healthy', 'Hernia']\n",
    "        label_mapping = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "        for class_name, label in label_mapping.items():\n",
    "            class_folder = os.path.join(dataset_folder, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_name in os.listdir(class_folder):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    if os.path.isfile(img_path):\n",
    "                        data.append(img_path)\n",
    "                        labels.append(label)\n",
    "\n",
    "        print(f\"Loaded {len(data)} images from {dataset_folder}.\")\n",
    "        return list(zip(data, labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load an image and its label\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate model parameters from multiple clients\n",
    "def aggregate_model_params(global_model, clients):\n",
    "    global_dict = global_model.state_dict()\n",
    "    client_dicts = [client[2].state_dict() for client in clients]\n",
    "    total_data_points = sum(len(client[0].dataset) for client in clients)\n",
    "\n",
    "    for key in global_dict.keys():\n",
    "        weighted_sum = torch.zeros_like(global_dict[key], dtype=torch.float32)\n",
    "        for client_data, client_dict in zip(clients, client_dicts):\n",
    "            weight = len(client_data[0].dataset) / total_data_points\n",
    "            weighted_sum += client_dict[key].float() * weight\n",
    "        global_dict[key] = weighted_sum.type(global_dict[key].dtype)\n",
    "\n",
    "    global_model.load_state_dict(global_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_global_model(model, val_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    # Final Tensor Concatenation\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro')(all_preds, all_labels).item()\n",
    "    precision = MulticlassPrecision(num_classes=num_classes, average='macro')(all_preds, all_labels).item()\n",
    "    recall = MulticlassRecall(num_classes=num_classes, average='macro')(all_preds, all_labels).item()\n",
    "    f1 = MulticlassF1Score(num_classes=num_classes, average='macro')(all_preds, all_labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "    return val_loss, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform federated training\n",
    "def federated_train(global_model, criterion, clients, val_loader, device, num_classes, num_epochs=100, patience=15):\n",
    "    best_accuracy = 0.0\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nFederated Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        global_model.train()\n",
    "        epoch_client_losses = []\n",
    "        epoch_client_accuracies = []\n",
    "\n",
    "        for client_id, (client_data_loader, client_optimizer, client_model, client_scheduler) in enumerate(clients):\n",
    "            client_model.load_state_dict(global_model.state_dict())\n",
    "            client_model.to(device)\n",
    "            client_model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            for images, labels in client_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                client_optimizer.zero_grad()\n",
    "                outputs = client_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                client_optimizer.step()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "            client_loss = running_loss / total_train if total_train > 0 else 0.0\n",
    "            client_accuracy = correct_train / total_train if total_train > 0 else 0.0\n",
    "            epoch_client_losses.append(client_loss)\n",
    "            epoch_client_accuracies.append(client_accuracy)\n",
    "            print(f\"Client {client_id + 1}: Loss: {client_loss:.4f}, Accuracy: {client_accuracy:.4f}\")\n",
    "\n",
    "        avg_loss = np.mean(epoch_client_losses)\n",
    "        avg_accuracy = np.mean(epoch_client_accuracies)\n",
    "        print(f\"Average Client Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Average Client Accuracy: {avg_accuracy:.4f}\")\n",
    "        aggregate_model_params(global_model, clients)\n",
    "        val_loss, accuracy, precision, recall, f1 = validate_global_model(global_model, val_loader, criterion, device, num_classes)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(global_model.state_dict(), \"best_model2.pth\")\n",
    "            print(\"Saved the best model based on validation accuracy!\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            print(f\"No improvement for {epochs_since_improvement} epochs.\")\n",
    "\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    print(\"Training completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61269 images from C:\\Users\\Acer\\Desktop\\Model\\ok\\train.\n",
      "Loaded 13131 images from C:\\Users\\Acer\\Desktop\\Model\\ok\\val.\n",
      "Loaded 13140 images from C:\\Users\\Acer\\Desktop\\Model\\ok\\test.\n",
      "\n",
      "Federated Epoch [1/100]\n",
      "Client 1: Loss: 0.3041, Accuracy: 0.3520\n",
      "Client 2: Loss: 0.3017, Accuracy: 0.3535\n",
      "Client 3: Loss: 0.3040, Accuracy: 0.3492\n",
      "Average Client Loss: 0.3033\n",
      "Average Client Accuracy: 0.3516\n",
      "Validation Loss: 0.3940 | Accuracy: 0.2506 | Precision: 0.2811 | Recall: 0.2506 | F1 Score: 0.1926\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [2/100]\n",
      "Client 1: Loss: 0.2582, Accuracy: 0.4078\n",
      "Client 2: Loss: 0.2618, Accuracy: 0.4018\n",
      "Client 3: Loss: 0.2611, Accuracy: 0.4007\n",
      "Average Client Loss: 0.2604\n",
      "Average Client Accuracy: 0.4035\n",
      "Validation Loss: 0.3012 | Accuracy: 0.3721 | Precision: 0.4528 | Recall: 0.3721 | F1 Score: 0.3313\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [3/100]\n",
      "Client 1: Loss: 0.2293, Accuracy: 0.4478\n",
      "Client 2: Loss: 0.2297, Accuracy: 0.4495\n",
      "Client 3: Loss: 0.2317, Accuracy: 0.4451\n",
      "Average Client Loss: 0.2302\n",
      "Average Client Accuracy: 0.4475\n",
      "Validation Loss: 0.2374 | Accuracy: 0.4716 | Precision: 0.5289 | Recall: 0.4716 | F1 Score: 0.4528\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [4/100]\n",
      "Client 1: Loss: 0.2091, Accuracy: 0.4762\n",
      "Client 2: Loss: 0.2101, Accuracy: 0.4738\n",
      "Client 3: Loss: 0.2076, Accuracy: 0.4762\n",
      "Average Client Loss: 0.2089\n",
      "Average Client Accuracy: 0.4754\n",
      "Validation Loss: 0.2273 | Accuracy: 0.4883 | Precision: 0.5412 | Recall: 0.4883 | F1 Score: 0.4681\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [5/100]\n",
      "Client 1: Loss: 0.1900, Accuracy: 0.5026\n",
      "Client 2: Loss: 0.1921, Accuracy: 0.4973\n",
      "Client 3: Loss: 0.1928, Accuracy: 0.4978\n",
      "Average Client Loss: 0.1917\n",
      "Average Client Accuracy: 0.4992\n",
      "Validation Loss: 0.2168 | Accuracy: 0.5114 | Precision: 0.5664 | Recall: 0.5114 | F1 Score: 0.4920\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [6/100]\n",
      "Client 1: Loss: 0.1783, Accuracy: 0.5212\n",
      "Client 2: Loss: 0.1786, Accuracy: 0.5182\n",
      "Client 3: Loss: 0.1801, Accuracy: 0.5165\n",
      "Average Client Loss: 0.1790\n",
      "Average Client Accuracy: 0.5186\n",
      "Validation Loss: 0.2003 | Accuracy: 0.5303 | Precision: 0.5769 | Recall: 0.5303 | F1 Score: 0.5181\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [7/100]\n",
      "Client 1: Loss: 0.1663, Accuracy: 0.5372\n",
      "Client 2: Loss: 0.1654, Accuracy: 0.5383\n",
      "Client 3: Loss: 0.1678, Accuracy: 0.5336\n",
      "Average Client Loss: 0.1665\n",
      "Average Client Accuracy: 0.5364\n",
      "Validation Loss: 0.1879 | Accuracy: 0.5510 | Precision: 0.5763 | Recall: 0.5510 | F1 Score: 0.5375\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [8/100]\n",
      "Client 1: Loss: 0.1535, Accuracy: 0.5573\n",
      "Client 2: Loss: 0.1546, Accuracy: 0.5571\n",
      "Client 3: Loss: 0.1585, Accuracy: 0.5479\n",
      "Average Client Loss: 0.1555\n",
      "Average Client Accuracy: 0.5541\n",
      "Validation Loss: 0.1867 | Accuracy: 0.5580 | Precision: 0.5979 | Recall: 0.5580 | F1 Score: 0.5434\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [9/100]\n",
      "Client 1: Loss: 0.1489, Accuracy: 0.5668\n",
      "Client 2: Loss: 0.1429, Accuracy: 0.5747\n",
      "Client 3: Loss: 0.1462, Accuracy: 0.5662\n",
      "Average Client Loss: 0.1460\n",
      "Average Client Accuracy: 0.5692\n",
      "Validation Loss: 0.2014 | Accuracy: 0.5336 | Precision: 0.5870 | Recall: 0.5336 | F1 Score: 0.5298\n",
      "No improvement for 1 epochs.\n",
      "\n",
      "Federated Epoch [10/100]\n",
      "Client 1: Loss: 0.1367, Accuracy: 0.5815\n",
      "Client 2: Loss: 0.1411, Accuracy: 0.5761\n",
      "Client 3: Loss: 0.1377, Accuracy: 0.5787\n",
      "Average Client Loss: 0.1385\n",
      "Average Client Accuracy: 0.5788\n",
      "Validation Loss: 0.1749 | Accuracy: 0.5751 | Precision: 0.6107 | Recall: 0.5751 | F1 Score: 0.5660\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [11/100]\n",
      "Client 1: Loss: 0.1303, Accuracy: 0.5930\n",
      "Client 2: Loss: 0.1259, Accuracy: 0.6008\n",
      "Client 3: Loss: 0.1304, Accuracy: 0.5930\n",
      "Average Client Loss: 0.1289\n",
      "Average Client Accuracy: 0.5956\n",
      "Validation Loss: 0.1617 | Accuracy: 0.5967 | Precision: 0.6228 | Recall: 0.5967 | F1 Score: 0.5902\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [12/100]\n",
      "Client 1: Loss: 0.1204, Accuracy: 0.6064\n",
      "Client 2: Loss: 0.1228, Accuracy: 0.6025\n",
      "Client 3: Loss: 0.1220, Accuracy: 0.6068\n",
      "Average Client Loss: 0.1217\n",
      "Average Client Accuracy: 0.6052\n",
      "Validation Loss: 0.1580 | Accuracy: 0.6047 | Precision: 0.6277 | Recall: 0.6047 | F1 Score: 0.5965\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [13/100]\n",
      "Client 1: Loss: 0.1147, Accuracy: 0.6169\n",
      "Client 2: Loss: 0.1124, Accuracy: 0.6226\n",
      "Client 3: Loss: 0.1146, Accuracy: 0.6185\n",
      "Average Client Loss: 0.1139\n",
      "Average Client Accuracy: 0.6193\n",
      "Validation Loss: 0.1630 | Accuracy: 0.6048 | Precision: 0.6341 | Recall: 0.6048 | F1 Score: 0.5927\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [14/100]\n",
      "Client 1: Loss: 0.1048, Accuracy: 0.6340\n",
      "Client 2: Loss: 0.1060, Accuracy: 0.6355\n",
      "Client 3: Loss: 0.1078, Accuracy: 0.6279\n",
      "Average Client Loss: 0.1062\n",
      "Average Client Accuracy: 0.6325\n",
      "Validation Loss: 0.1494 | Accuracy: 0.6238 | Precision: 0.6380 | Recall: 0.6238 | F1 Score: 0.6163\n",
      "Saved the best model based on validation accuracy!\n",
      "\n",
      "Federated Epoch [15/100]\n",
      "Client 1: Loss: 0.1001, Accuracy: 0.6441\n",
      "Client 2: Loss: 0.0976, Accuracy: 0.6478\n"
     ]
    }
   ],
   "source": [
    "# Main function to load data and start federated training\n",
    "def main():\n",
    "    train_folder = \"C:\\\\Users\\\\Acer\\\\Desktop\\\\Model\\\\ok\\\\train\"\n",
    "    val_folder = \"C:\\\\Users\\\\Acer\\\\Desktop\\\\Model\\\\ok\\\\val\"\n",
    "    test_folder = \"C:\\\\Users\\\\Acer\\\\Desktop\\\\Model\\\\ok\\\\test\"\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = LungImageDataset(dataset_folder=train_folder, transform=train_transform)\n",
    "    val_dataset = LungImageDataset(dataset_folder=val_folder, transform=val_test_transform)\n",
    "    test_dataset = LungImageDataset(dataset_folder=test_folder, transform=val_test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    num_classes = 15\n",
    "    global_model = CustomResNet(num_classes=num_classes).to(device)\n",
    "    class_weights_tensor = torch.ones(num_classes, dtype=torch.float32).to(device)\n",
    "    criterion = FocalLoss(class_weights=class_weights_tensor)\n",
    "\n",
    "    num_clients = 3\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        client_model = CustomResNet(num_classes=num_classes)\n",
    "        client_optimizer = optim.Adam(client_model.parameters(), lr=0.001)\n",
    "        client_scheduler = optim.lr_scheduler.StepLR(client_optimizer, step_size=5, gamma=0.1)\n",
    "        client_loader, _ = random_split(train_dataset, [len(train_dataset) // num_clients, len(train_dataset) - (len(train_dataset) // num_clients)])\n",
    "        client_loader = DataLoader(client_loader, batch_size=32, shuffle=True)\n",
    "        clients.append((client_loader, client_optimizer, client_model, client_scheduler))\n",
    "\n",
    "    federated_train(global_model, criterion, clients, val_loader, device, num_classes, num_epochs=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
